{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#EDA\" data-toc-modified-id=\"EDA-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>EDA</a></span><ul class=\"toc-item\"><li><span><a href=\"#exploring-&quot;SCIENCE_DATA&quot;\" data-toc-modified-id=\"exploring-&quot;SCIENCE_DATA&quot;-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>exploring \"SCIENCE_DATA\"</a></span><ul class=\"toc-item\"><li><span><a href=\"#right,-i-forgot-there's-a-readme-explaining-everything-quite-clearly.\" data-toc-modified-id=\"right,-i-forgot-there's-a-readme-explaining-everything-quite-clearly.-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>right, i forgot there's a readme explaining everything quite clearly.</a></span></li><li><span><a href=\"#so-how-do-i-interpret-the-320x36?\" data-toc-modified-id=\"so-how-do-i-interpret-the-320x36?-1.1.2\"><span class=\"toc-item-num\">1.1.2&nbsp;&nbsp;</span>so how do i interpret the 320x36?</a></span><ul class=\"toc-item\"><li><span><a href=\"#before-i-forget,-there's-probably-a-library-to-parse-lat/long-into-geographic-regions/cities/countries\" data-toc-modified-id=\"before-i-forget,-there's-probably-a-library-to-parse-lat/long-into-geographic-regions/cities/countries-1.1.2.1\"><span class=\"toc-item-num\">1.1.2.1&nbsp;&nbsp;</span>before i forget, there's probably a library to parse lat/long into geographic regions/cities/countries</a></span></li></ul></li></ul></li><li><span><a href=\"#Data-format-is-based-off-of-instrument-format\" data-toc-modified-id=\"Data-format-is-based-off-of-instrument-format-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Data format is based off of instrument format</a></span></li><li><span><a href=\"#filename-considerations\" data-toc-modified-id=\"filename-considerations-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>filename considerations</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fo = open(\"foo.txt\", \"r+\")\n",
    "str = fo.read(10);\n",
    "print \"Read String is : \", str\n",
    "# Close opend file\n",
    "fo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fo.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H\n"
     ]
    }
   ],
   "source": [
    "fo = open(filename, 'r+', encoding = \"ISO-8859-1\")\n",
    "st = fo.read(2)\n",
    "print(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "D\n",
      "F\n",
      "\n",
      "\u001a\n",
      "\u0000\n",
      "\u0000\u0000\u0000\u0000\n",
      "\b\b\u0000\u0004\u0000\n",
      "\u0010\u0000\u0000\u0000\u0000\u0000\n",
      "\u0000\u0000\u0000\u0000\u0000\u0000\u0000\n",
      "\u0000ÿÿÿÿÿÿÿ\n",
      "ÿ\u0012\u0000\u0000\u0000\u0000\u0000\n",
      "ÿÿÿÿÿÿÿÿ\u0000\u0000\n",
      "\u0000\u0000\u0000\u0000\u0000\u0000`\u0000\u0000\u0000\u0000\n",
      "\u0000\u0000\u0000\u0001\u0000\u0000\u0000\u0000\u0000\u0000\u0000\n",
      "\u0000\u0000\u0000\u0000\u0000\u0000\u0000¨\u0002\u0000\u0000\u0000\u0000\n",
      "\u0000\u0000\u0001\u0000,\u0000\u0001\u0000\u0000\u0000\u0018\u0000\u0000\u0000\n",
      "\u0000\u0000\u0000\u0000\u0010\u0000\u0010\u0000\u0000\u0000\u0000\u0000£v\u0012\n",
      "\u0000\u0000\u0000\u0000\u0000è\f",
      "\u0000\u0000\u0000\u0000\u0000\u0000TRE\n",
      "E\u0000\u0000\u0001\u0000ÿÿÿÿÿÿÿÿÿÿÿÿ\n",
      "ÿÿÿÿ\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u00000\u0004\u0000\u0000\u0000\u0000\n",
      "\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\n",
      "\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\n",
      "\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\n",
      "\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\n",
      "\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\n",
      "\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\n",
      "\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\n",
      "\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\n",
      "\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\n",
      "\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\n",
      "\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\n",
      "\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\n",
      "\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\n",
      "\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\n",
      "\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\n",
      "\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\n",
      "\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\n",
      "\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\n",
      "\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000HEAP\u0000\u0000\u0000\u0000°\u0000\u0000\u0000\u0000\u0000\u0000\u0000`\u0000\u0000\u0000\u0000\u0000\u0000\u00001N\n",
      "\u0001\u0000\u0000\u0000\u0000\u0000\f",
      "\u00000\u0000\u0000\u0000\u0000\u0000\u0001\u0000\n",
      "\u0000\b\u0000\b\u0000long_name\u0000\u0000\u0000\u0000\u0000\u0000\u0000\n",
      "\u0013\u0000\u0000\u0000\b\u0000\u0000\u0000\u0001\u0000\u0000\u0000\u0000\u0000\u0000\u0000Latitude\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\n",
      "\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0001\u0000\u0010\u0000\u0001\u0000\u0000\u0000\u0000\u0001\u0000\u0000\u0000\u0000\u0000\u0000\u0001\u0000\u0018\u0000\u0000\u0000\u0000\n",
      "\u0000\u0001\u0001\u0001\u0000\u0000\u0000\u0000\u0000\u0001\u0000\u0000\u0000\u0000\u0000\u0000\u0001\u0000\u0000\u0000\u0000\u0000\u0000\u0010\u0000\u0010\u0000\u0000\u0000\u0000\u0000x\u0005\u0000\u0000\u0000\u0000\u0000\u0000\n",
      "(\u0001\u0000\u0000\u0000\u0000\u0000\u0000\u0005\u0000\u0010\u0000\u0001\u0000\u0000\u0000\u0002\u0003\u0002\u0001\u0004\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0004\u0000\b\u0000\u0001\u0000\u0000\u0000\u0004\u0000\n",
      "\u0000\u0000\u0000\u0000\u0000\u0000\u000b",
      "\u0000 \u0000\u0001\u0000\u0000\u0000\u0001\u0001\u0000\u0000\u0000\u0000\u0000\u0000\u0001\u0000\b\u0000\u0001\u0000\u0001\u0000deflate\u0000\u0005\u0000\u0000\u0000\u0000\n",
      "\u0000\u0000\u0000\b\u0000\u0018\u0000\u0001\u0000\u0000\u0000\u0003\u0002\u0002 \u0006\u0000\u0000\u0000\u0000\u0000\u0000\u0001\u0000\u0000\u0004\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0012\u0000\b\u0000\u0000\u0000\u0000\u0000\u0001\n",
      "\u0000\u0000\u0000wÄ]\f",
      "\u00000\u0000\u0000\u0000\u0000\u0000\u0001\u0000\u0006\u0000\b\u0000\b\u0000CLASS\u0000\u0000\u0000\u0013\u0000\u0000\u0000\u0010\u0000\u0000\u0000\u0001\u0000\u0000\u0000\u0000\u0000\n",
      "\u0000\u0000DIMENSION_SCALE\u0000\u0000\u0000\b\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000SNOD\u0001\u0000\u0007\u0000(\u0000\u0000\u0000\n",
      "\u0000\u0000\u0000\u0000\u0010.\u0000\u0000\u0000\u0000\u0000\u0000\u0001\u0000\u0000\u0000\u0000\u0000\u0000\u00008.\u0000\u0000\u0000\u0000\u0000\u0000X0\u0000\u0000\u0000\u0000\u0000\u00008\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000E\u0000\n",
      "\u0000\u0000\u0000\u0000\u0000\u0001\u0000\u0000\u0000\u0000\u0000\u0000\u0000K\u0001\u0000\u0000\u0000\u0000\u0000¹M\u0001\u0000\u0000\u0000\u0000\u0000P\u0000\u0000\u0000\u0000\u0000\u0000\u0000¨è\u0004\u0000\u0000\u0000\u0000\u0000\u0001\u0000\u0000\n",
      "\u0000\u0000\u0000\u0000\u0000r\u0006\u0000\u0000\u0000\u0000\u0000¡\u0006\u0000\u0000\u0000\u0000\u0000\b\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0003\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\n"
     ]
    }
   ],
   "source": [
    "for i in range(50):\n",
    "    s = fo.read(i)\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "filename = \"many_outs.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Unable to open file (truncated file: eof = 622592, sblock->base_addr = 0, stored_eof = 1213323)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-27e0cdecf6ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;31m# List all groups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Keys: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0ma_group_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, **kwds)\u001b[0m\n\u001b[1;32m    392\u001b[0m                 fid = make_fid(name, mode, userblock_size,\n\u001b[1;32m    393\u001b[0m                                \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmake_fcpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m                                swmr=swmr)\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to open file (truncated file: eof = 622592, sblock->base_addr = 0, stored_eof = 1213323)"
     ]
    }
   ],
   "source": [
    "with h5py.File(filename, \"r\",) as f:\n",
    "    # List all groups\n",
    "    print(\"Keys: %s\" % f.keys())\n",
    "    a_group_key = list(f.keys())[0]\n",
    "\n",
    "    # Get the data\n",
    "    data = list(f[a_group_key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "okay, found the issue\n",
    "- for downloading multiple .h5 files, nasa reccomends:\n",
    "    - cat <url.txt> | tr -d '\\r' | xargs -n 1 curl -LJO -n -c ~/.urs_cookies -b ~/.urs_cookies\n",
    "    - but i was getting some access denied + use --output recommendations. didn't work\n",
    "        - so i tried >> piping it to a file. i could have named it \"data.h5\", maybe? can you store h5s in other h5s?\n",
    "        - i named it 'data.iso' on someone's SO solution\n",
    "        - this went poorly. the data downloaded, but when trying to open it, it seemed super corrupted, even with specific encoding that allowed it to open\n",
    "    - i went back to NASA's downloading instructions and tested the curl cmd with a smaller subset of .h5 files, which worked (i think i had some typos in the cmdline, or i was formatting incorrectly) so i applied it to the original 2012-2020 46gb of data. currently downloading all the .h5s to my home directory, so i'll just have to move them en masse and loop-open them into a dataframe in the project folder (in 5 hours when it downloads)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(filename, \"r\",) as f:\n",
    "    # List all groups\n",
    "    print(\"Keys: %s\" % f.keys())\n",
    "    a_group_key = list(f.keys())[0]\n",
    "\n",
    "    # Get the data\n",
    "    data = list(f[a_group_key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# for filename in os.listdir(os.getcwd()):\n",
    "#    with open(os.path.join(os.cwd(), filename), 'r') as f: # open in readonly mode\n",
    "#       # do your stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# lst = []\n",
    "# for filename in os.listdir(os.getcwd()):\n",
    "#     if filename.endswith('.h5'):\n",
    "#         with h5py.File(filename, 'r') as f:\n",
    "#             print('keys: %s' %f.keys())\n",
    "#             a_group_key = list(f.keys())[0]\n",
    "#             data = list(f[a_group_key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File('one.h5', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KeysViewHDF5 ['ANCILLARY_DATA', 'GEOLOCATION_DATA', 'SCIENCE_DATA', 'nTimes', 'nWavel2', 'nWavel3', 'nXtrack']>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANCILLARY_DATA <class 'h5py._hl.group.Group'> \n",
      "\n",
      "GEOLOCATION_DATA <class 'h5py._hl.group.Group'> \n",
      "\n",
      "SCIENCE_DATA <class 'h5py._hl.group.Group'> \n",
      "\n",
      "nTimes <class 'h5py._hl.dataset.Dataset'> \n",
      "\n",
      "nWavel2 <class 'h5py._hl.dataset.Dataset'> \n",
      "\n",
      "nWavel3 <class 'h5py._hl.dataset.Dataset'> \n",
      "\n",
      "nXtrack <class 'h5py._hl.dataset.Dataset'> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k in f.keys():\n",
    "    print(k, type(f[k]), '\\n') # remember to use the key on the dict obj to actually get the type of value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- so the root h5py object is a dictionary... of sorts.\n",
    "    - we've got Groups, which are like folders, and Datasets, which are like arrays (but different)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KeysViewHDF5 ['Latitude', 'Longitude', 'SolarAzimuthAngle', 'SolarZenithAngle', 'SpacecraftAltitude', 'SpacecraftLatitude', 'SpacecraftLongitude', 'TimeTAI93', 'TimeUTC', 'ViewingAzimuthAngle', 'ViewingZenithAngle']>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f['GEOLOCATION_DATA'].keys() # looks like location of Suomi satellite at moment of reading\n",
    "# we're gonna want Latitude and Longitude to identify 'where' the reading is.\n",
    "# maybe TimeUTC as well, that could save a lot of re-coding work for broad regional targeting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KeysViewHDF5 ['CloudPressure', 'TerrainPressure']>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f['ANCILLARY_DATA'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KeysViewHDF5 ['CloudFraction', 'ColumnAmountO3', 'ColumnAmountSO2_PBL', 'ColumnAmountSO2_STL', 'ColumnAmountSO2_TRL', 'ColumnAmountSO2_TRM', 'ColumnAmountSO2_TRU', 'FittingWindow_STL', 'FittingWindow_TRL', 'FittingWindow_TRM', 'FittingWindow_TRU', 'Flag_SAA', 'Flag_SO2', 'RadiativeCloudFraction', 'Reflectivity331', 'SLER', 'UVAerosolIndex', 'Wavelengths_SLER', 'dNdR', 'nPrincipalComponents']>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f['SCIENCE_DATA'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from the NASA dataset description:\n",
    "\n",
    "The goal of this dataset is to create and archive a Level 2 Sulfur Dioxide (SO2) Earth Science Data Record (ESDR) from backscatter Ultraviolet (BUV) measurements from the Ozone Mapping and Profiler Suite (OMPS) Nadir Mapper (NM) flown on the NASA/NOAA Suomi National Polar-orbiting Partnership (S-NPP) satellite since 2011. We apply the NASA Goddard Space Flight Center (GSFC) principal component analysis (PCA) satellite trace gas retrieval algorithm, to obtain the best measurement-based ESDR of volcanic and anthropogenic SO2 loadings and emissions, which is fully consistent with our SO2 ESDR (OMSO2) from the Ozone Monitoring Instrument (OMI) flown on the NASA Earth Observing System (EOS) Aura satellite since 2004. \n",
    "\n",
    "The SO2 ESDR released here at the Goddard Earth Science (GES) Data and Information Data Center (DISC), OMPS_NPP_NMSO2_PCA_L2, is a stand-alone dataset, but will also be part of the NASA's Making Earth System Data Records for Use in Research Environments (MEaSUREs) program. It covers the entire period of the S-NPP/OMPS mission since launch in October 2011, and forward data processing is ongoing. OMPS_NPP_NMSO2_PCA_L2 is a Level 2 orbital swath product, which will be used to study SO2 pollutants emitted from large point sources such as coal-fired power plants and smelters as well as to monitor volcanic SO2 emissions.\n",
    "\n",
    "Sulfur Dioxide (SO2) is a short-lived gas primarily produced by volcanoes, power plants, refineries, metal smelting and burning of fossil fuels. Where SO2 remains near the Earth's surface, it is toxic, causes acid rain, and degrades air quality. Where SO2 is lofted into the free troposphere, it forms aerosols that can alter cloud reflectivity and precipitation. In the stratosphere, volcanic SO2 forms sulfate aerosols that can result in climate change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- translation:\n",
    "    - here's SO2 levels by latitude, date & many other variables\n",
    "        - derived from applying PCA algos to Ozone mapping data, which was measured from backscatter ultraviolet readings\n",
    "                - basically, NASA looks at backscattered UV off of the atmosphere - off of ozone, but can effectively derive SO2 from the ozone reading using PCA\n",
    "        - seems convoluted, but it's consistent with their last SO2 reading system from 2004, and it's probably the best estimate the world's got right now (unless anyone would like to step up and accurately measure trace gas concentrations globally every day. seems bloody hard in my opinion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## exploring \"SCIENCE_DATA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KeysViewHDF5 ['CloudFraction', 'ColumnAmountO3', 'ColumnAmountSO2_PBL', 'ColumnAmountSO2_STL', 'ColumnAmountSO2_TRL', 'ColumnAmountSO2_TRM', 'ColumnAmountSO2_TRU', 'FittingWindow_STL', 'FittingWindow_TRL', 'FittingWindow_TRM', 'FittingWindow_TRU', 'Flag_SAA', 'Flag_SO2', 'RadiativeCloudFraction', 'Reflectivity331', 'SLER', 'UVAerosolIndex', 'Wavelengths_SLER', 'dNdR', 'nPrincipalComponents']>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f['SCIENCE_DATA'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"ColumnAmountSO2_PBL\": shape (320, 36), type \"<f4\">"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f['SCIENCE_DATA']['ColumnAmountSO2_PBL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbl = f['SCIENCE_DATA']['ColumnAmountSO2_PBL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def peek_dataset(ds): # only works on 2d array datasets\n",
    "    for i in range(ds.shape[0]):\n",
    "        for k in range(ds.shape[1]):\n",
    "            print(ds[i][k])\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# peek_dataset(pbl) # a 320x36 matrix. now to figure out why there's 11520 readings in this dataset, which is one .h5 file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ColumnAmountSO2_PBL (320, 36)\n",
      "ColumnAmountSO2_STL (320, 36)\n",
      "ColumnAmountSO2_TRL (320, 36)\n",
      "ColumnAmountSO2_TRM (320, 36)\n",
      "ColumnAmountSO2_TRU (320, 36)\n",
      "Flag_SO2 (320, 36)\n"
     ]
    }
   ],
   "source": [
    "for k in f['SCIENCE_DATA'].keys(): # are they all 320x36? probably the SO2 at least\n",
    "#     print(k, type(k))\n",
    "    if 'SO2' in k: # k is a string\n",
    "        print(k, f['SCIENCE_DATA'][k].shape) # yes, the SO2 readings are matched in size.\n",
    "# probably different instruments or estimation methods. perhaps we could aggregate them later on, but they're all going in the dataframe for now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### right, i forgot there's a readme explaining everything quite clearly.\n",
    "- units\n",
    "    - 5 estimates (principal components), related to various geophysical processes (ozone absorption, rotational Raman scattering) and instrument measurement details (wavelength shift, dark current)\n",
    "    - PCs used in SO2 spectral fitting to reduce interferences\n",
    "        - so we've got five estimates of total \"SO2 Vertical Column Density\" in Dobson Units (1DU = 2.69 * 10^16 molecules/cm^2)\n",
    "            - note the \"cm^2\". these are flat \"slices\" of a column? no\n",
    "            - each of the 5 estimates corresponds to a different assumed SO2 cloud height, or Center of Mass Altitude (CMA)\n",
    "            - all 5 VCD values represent the estimated total SO2 burden within entire atmospheric column:\n",
    "                - should not be interpreted as partial column amounts within different layers/parts of atmosphere\n",
    "                - they want users to select 1 or interpolate between two estimates that are most representative of conditions for a particular case of interest\n",
    "- so each of the 5 estimates measures with different heights/sensitivities, and therefore are 'tuned' to measure different... types of SO2 emission.\n",
    "    - anthropogenic (coal factories etc) SO2 emissions will look different, and be measured differently from volcanic eruptions\n",
    "    - that sort of thing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- PBL: Planetary Boundary Layer\n",
    "    - assumes that SO2 is predominantly in lowest 1km of atmosphere\n",
    "    - use for studies on near-surface pollution\n",
    "        - for each OMPS orbit:\n",
    "            - process 36 rows (cross-track positions) one at a time\n",
    "            - use PCA to extract PCs for spectral range 310.5-340nm from sun-normalized BUV radiance spectra\n",
    "            - PCs are ranked in descending order according to spectral variance that they explain\n",
    "    - due to jacobian simplication, they recommend only using snow/ice free pixels with small radiative cloud fraction\n",
    "- the other 4 column amounts seem to be volcanic\n",
    "### so how do i interpret the 320x36?\n",
    "- wait, they're talking about pixels\n",
    "    - 320x36 is the \"resolution\" of the measurement\n",
    "    \n",
    "#### before i forget, there's probably a library to parse lat/long into geographic regions/cities/countries\n",
    "\n",
    "- it's an \"orbital-track\" product about the SNPP satellite.\n",
    "    - polar sun-synchronous orbit\n",
    "## Data format is based off of instrument format\n",
    "- OMPS-NM has 110° FoV, cross-track swath ~= 2800km\n",
    "    - global coverage,14-15 orbits/day\n",
    "    - \"nominal spatial resolution\" is 50km x 50km at nadir in nominal observation mode\n",
    "        - there's a higher res mode recorded once a week, but that's not in this dataset\n",
    "- for each OMPS orbit, process 36 rows (cross-track positions) use PCA to extract PCs for spectral range from BUV\n",
    "    - so, 320 PCs per position while crossing orbital track?\n",
    "        - what does the lat/long data look like? that might solve this question\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat = f['GEOLOCATION_DATA']['Latitude']\n",
    "long = f['GEOLOCATION_DATA']['Longitude']\n",
    "print(lat.shape, long.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- so in one .h5's 36x320:\n",
    "    - began at -29, -152: about 1/3 of the way from NZ to Ecuador\n",
    "    - ended at 67, 16: northern middle Norway\n",
    "        - but those aren't the extreme bounds for lat or long.\n",
    "            - without plotting it out, my intuition says it's an oscillating path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# peek_dataset(lat)\n",
    "# peek_dataset(long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- now the 50km spatial resolution makes sense.\n",
    "    - we've got 11520 'pixels' or 'granules' (are granules the .h5?)\n",
    "    - each pixel has an SO2 PBL reading, as well as a lat/long\n",
    "        - so the corresponding [x],[y] position in the matrix will match a geolocation to its PBL\n",
    "- warnings from the readme:\n",
    "    - all pixels:\n",
    "        - with large solar zenith angle (SZA>70)\n",
    "        - near edge of swath (rows 1-2, 35-36)\n",
    "        - affected by South Atlantic Anomaly (flag_SAA=1)\n",
    "            - should be excluded\n",
    "    - PBL SO2:\n",
    "        - only use:\n",
    "            - snow/ice free pixels\n",
    "            - with RadiativeCloudFraction (<0.3)\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## filename considerations\n",
    "- each .h5 file contains:\n",
    "    - date/time at start of orbit\n",
    "    - orbit number\n",
    "        - so each .h5 is an orbit. question answered\n",
    "- what about time? that should help me figure out \"unique positions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(320,)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geo = f['GEOLOCATION_DATA']\n",
    "time = geo['TimeUTC']\n",
    "time.shape # ah"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- so we've got 320 times per orbit, but 320x36 pixels and 320x36 lat/long coords\n",
    "- that means at 320 times throughout the day, the satellite gathers 36 pixels, and each one has a unique geolocation...?\n",
    "    - the 36 is \"cross-track positions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## right, i think i've got the format\n",
    "- each .h5 file is an orbit, with 36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(320,)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KeysViewHDF5 ['Latitude', 'Longitude', 'SolarAzimuthAngle', 'SolarZenithAngle', 'SpacecraftAltitude', 'SpacecraftLatitude', 'SpacecraftLongitude', 'TimeTAI93', 'TimeUTC', 'ViewingAzimuthAngle', 'ViewingZenithAngle']>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f['GEOLOCATION_DATA'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(320, 36)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lat = f['GEOLOCATION_DATA']['Latitude']\n",
    "lat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KeysViewHDF5 ['CloudPressure', 'TerrainPressure']>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f['ANCILLARY_DATA'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(320,) int32\n"
     ]
    }
   ],
   "source": [
    "nt = f['nTimes']\n",
    "print(nt.shape, nt.dtype) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- this \"nTimes\" key has 320 ints lined up 1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
