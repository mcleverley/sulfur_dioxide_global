{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#htm-applications\" data-toc-modified-id=\"htm-applications-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>htm applications</a></span><ul class=\"toc-item\"><li><span><a href=\"#below-is-the-example-from-github-'hotgym.py'\" data-toc-modified-id=\"below-is-the-example-from-github-'hotgym.py'-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>below is the example from github 'hotgym.py'</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# htm applications\n",
    "- lots of NLP using semantic inference\n",
    "    - cortical has a neat API that does this for us\n",
    "- time series forecasting like intelletic\n",
    "    - we'll probably use this for our SO2 since it's geotemporal\n",
    "- predict SO2 for a given date/time, since we have lots of data to train it on\n",
    "- then compare real stuff against covid so2 emissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- there's a lot of other libraries on numenta for time series actually\n",
    "    - since we have 11 variables, and numenta often predicts X with X best instead of X with Y\n",
    "    - use swarming to test multivariate prediction combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## below is the example from github 'hotgym.py'\n",
    "- it runs into some weird order of operations error, maybe i need to rebuild using source instead of pip install htmcore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import datetime\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "\n",
    "from htm.bindings.sdr import SDR, Metrics\n",
    "from htm.encoders.rdse import RDSE, RDSE_Parameters\n",
    "from htm.encoders.date import DateEncoder\n",
    "from htm.bindings.algorithms import SpatialPooler\n",
    "from htm.bindings.algorithms import TemporalMemory\n",
    "from htm.algorithms.anomaly_likelihood import AnomalyLikelihood #FIXME use TM.anomaly instead, but it gives worse results than the py.AnomalyLikelihood now\n",
    "from htm.bindings.algorithms import Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/mark/Documents/flatiron/projects/so2/sulfur_dioxide_global/gymdata.csv'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "direc = os.getcwd()\n",
    "filepath = os.path.join(direc, 'gymdata.csv')\n",
    "filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "  # there are 2 (3) encoders: \"value\" (RDSE) & \"time\" (DateTime weekend, timeOfDay)\n",
    " 'enc': {\n",
    "      \"value\" :\n",
    "         {'resolution': 0.88, 'size': 700, 'sparsity': 0.02},\n",
    "      \"time\": \n",
    "         {'timeOfDay': (30, 1), 'weekend': 21}\n",
    " },\n",
    " 'predictor': {'sdrc_alpha': 0.1},\n",
    " 'sp': {'boostStrength': 3.0,\n",
    "        'columnCount': 1638,\n",
    "        'localAreaDensity': 0.04395604395604396,\n",
    "        'potentialPct': 0.85,\n",
    "        'synPermActiveInc': 0.04,\n",
    "        'synPermConnected': 0.13999999999999999,\n",
    "        'synPermInactiveDec': 0.006},\n",
    " 'tm': {'activationThreshold': 17,\n",
    "        'cellsPerColumn': 13,\n",
    "        'initialPerm': 0.21,\n",
    "        'maxSegmentsPerCell': 128,\n",
    "        'maxSynapsesPerSegment': 64,\n",
    "        'minThreshold': 10,\n",
    "        'newSynapseCount': 32,\n",
    "        'permanenceDec': 0.1,\n",
    "        'permanenceInc': 0.1},\n",
    " 'anomaly': {\n",
    "   'likelihood': \n",
    "       {#'learningPeriod': int(math.floor(self.probationaryPeriod / 2.0)),\n",
    "        #'probationaryPeriod': self.probationaryPeriod-default_parameters[\"anomaly\"][\"likelihood\"][\"learningPeriod\"],\n",
    "        'probationaryPct': 0.1,\n",
    "        'reestimationPeriod': 100} #These settings are copied from NAB\n",
    " }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = [] # read input file\n",
    "with open(filepath, 'r') as fin:\n",
    "    reader = csv.reader(fin)\n",
    "    headers = next(reader)\n",
    "    next(reader)\n",
    "    next(reader)\n",
    "    for record in reader:\n",
    "        records.append(record)\n",
    "        \n",
    "dateEncoder = DateEncoder(timeOfDay= parameters[\"enc\"][\"time\"][\"timeOfDay\"], \n",
    "                        weekend  = parameters[\"enc\"][\"time\"][\"weekend\"]) \n",
    "\n",
    "scalarEncoderParams            = RDSE_Parameters()\n",
    "scalarEncoderParams.size       = parameters[\"enc\"][\"value\"][\"size\"]\n",
    "scalarEncoderParams.sparsity   = parameters[\"enc\"][\"value\"][\"sparsity\"]\n",
    "scalarEncoderParams.resolution = parameters[\"enc\"][\"value\"][\"resolution\"]\n",
    "scalarEncoder = RDSE( scalarEncoderParams )\n",
    "encodingWidth = (dateEncoder.size + scalarEncoder.size)\n",
    "enc_info = Metrics( [encodingWidth], 999999999 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the HTM.  SpatialPooler & TemporalMemory & associated tools.\n",
    "spParams = parameters[\"sp\"]\n",
    "sp = SpatialPooler(\n",
    "        inputDimensions            = (encodingWidth,),\n",
    "        columnDimensions           = (spParams[\"columnCount\"],),\n",
    "        potentialPct               = spParams[\"potentialPct\"],\n",
    "        potentialRadius            = encodingWidth,\n",
    "        globalInhibition           = True,\n",
    "        localAreaDensity           = spParams[\"localAreaDensity\"],\n",
    "        synPermInactiveDec         = spParams[\"synPermInactiveDec\"],\n",
    "        synPermActiveInc           = spParams[\"synPermActiveInc\"],\n",
    "        synPermConnected           = spParams[\"synPermConnected\"],\n",
    "        boostStrength              = spParams[\"boostStrength\"],\n",
    "        wrapAround                 = True\n",
    ")\n",
    "sp_info = Metrics( sp.getColumnDimensions(), 999999999 )\n",
    "\n",
    "tmParams = parameters[\"tm\"]\n",
    "tm = TemporalMemory(\n",
    "        columnDimensions          = (spParams[\"columnCount\"],),\n",
    "        cellsPerColumn            = tmParams[\"cellsPerColumn\"],\n",
    "        activationThreshold       = tmParams[\"activationThreshold\"],\n",
    "        initialPermanence         = tmParams[\"initialPerm\"],\n",
    "        connectedPermanence       = spParams[\"synPermConnected\"],\n",
    "        minThreshold              = tmParams[\"minThreshold\"],\n",
    "        maxNewSynapseCount        = tmParams[\"newSynapseCount\"],\n",
    "        permanenceIncrement       = tmParams[\"permanenceInc\"],\n",
    "        permanenceDecrement       = tmParams[\"permanenceDec\"],\n",
    "        predictedSegmentDecrement = 0.0,\n",
    "        maxSegmentsPerCell        = tmParams[\"maxSegmentsPerCell\"],\n",
    "        maxSynapsesPerSegment     = tmParams[\"maxSynapsesPerSegment\"]\n",
    ")\n",
    "tm_info = Metrics( [tm.numberOfCells()], 999999999 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup likelihood, these settings are used in NAB\n",
    "anParams = parameters[\"anomaly\"][\"likelihood\"]\n",
    "probationaryPeriod = int(math.floor(float(anParams[\"probationaryPct\"])*len(records)))\n",
    "learningPeriod     = int(math.floor(probationaryPeriod / 2.0))\n",
    "anomaly_history = AnomalyLikelihood(learningPeriod= learningPeriod,\n",
    "                              estimationSamples= probationaryPeriod - learningPeriod,\n",
    "                              reestimationPeriod= anParams[\"reestimationPeriod\"])\n",
    "\n",
    "predictor = Predictor( steps=[1, 5], alpha=parameters[\"predictor\"]['sdrc_alpha'] )\n",
    "predictor_resolution = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through every datum in the dataset, record the inputs & outputs.\n",
    "inputs      = []\n",
    "anomaly     = []\n",
    "anomalyProb = []\n",
    "predictions = {1: [], 5: []}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CHECK FAILED: \"dimensions_ != 0\" Classifier: must call `learn` before `infer`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-316e496afa2a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m# Predict what will happen, and then train the predictor based on what just happened.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mpdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetActiveCells\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CHECK FAILED: \"dimensions_ != 0\" Classifier: must call `learn` before `infer`."
     ]
    }
   ],
   "source": [
    "for count, record in enumerate(records):\n",
    "\n",
    "    # Convert date string into Python date object.\n",
    "    dateString = datetime.datetime.strptime(record[0], \"%m/%d/%y %H:%M\")\n",
    "    # Convert data value string into float.\n",
    "    consumption = float(record[1])\n",
    "    inputs.append( consumption )\n",
    "\n",
    "    # Call the encoders to create bit representations for each value.  These are SDR objects.\n",
    "    dateBits        = dateEncoder.encode(dateString)\n",
    "    consumptionBits = scalarEncoder.encode(consumption)\n",
    "\n",
    "    # Concatenate all these encodings into one large encoding for Spatial Pooling.\n",
    "    encoding = SDR( encodingWidth ).concatenate([consumptionBits, dateBits])\n",
    "    enc_info.addData( encoding )\n",
    "\n",
    "    # Create an SDR to represent active columns, This will be populated by the\n",
    "    # compute method below. It must have the same dimensions as the Spatial Pooler.\n",
    "    activeColumns = SDR( sp.getColumnDimensions() )\n",
    "\n",
    "    # Execute Spatial Pooling algorithm over input space.\n",
    "    sp.compute(encoding, True, activeColumns)\n",
    "    sp_info.addData( activeColumns )\n",
    "\n",
    "    # Execute Temporal Memory algorithm over active mini-columns.\n",
    "    tm.compute(activeColumns, learn=True)\n",
    "    tm_info.addData( tm.getActiveCells().flatten() )\n",
    "\n",
    "    # Predict what will happen, and then train the predictor based on what just happened.\n",
    "    pdf = predictor.infer( tm.getActiveCells() )\n",
    "    for n in (1, 5):\n",
    "        if pdf[n]:\n",
    "            predictions[n].append( np.argmax( pdf[n] ) * predictor_resolution )\n",
    "        else:\n",
    "            predictions[n].append(float('nan'))\n",
    "\n",
    "    anomalyLikelihood = anomaly_history.anomalyProbability( consumption, tm.anomaly )\n",
    "    anomaly.append( tm.anomaly )\n",
    "    anomalyProb.append( anomalyLikelihood )\n",
    "\n",
    "    predictor.learn(count, tm.getActiveCells(), int(consumption / predictor_resolution))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print information & statistics about the state of the HTM.\n",
    "print(\"Encoded Input\", enc_info)\n",
    "print(\"\")\n",
    "print(\"Spatial Pooler Mini-Columns\", sp_info)\n",
    "print(str(sp))\n",
    "print(\"\")\n",
    "print(\"Temporal Memory Cells\", tm_info)\n",
    "print(str(tm))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # Shift the predictions so that they are aligned with the input they predict.\n",
    "for n_steps, pred_list in predictions.items():\n",
    "    for x in range(n_steps):\n",
    "        pred_list.insert(0, float('nan'))\n",
    "        pred_list.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy         = {1: 0, 5: 0}\n",
    "accuracy_samples = {1: 0, 5: 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, inp in enumerate(inputs):\n",
    "    for n in predictions: # For each [N]umber of time steps ahead which was predicted.\n",
    "        val = predictions[n][ idx ]\n",
    "        if not math.isnan(val):\n",
    "            accuracy[n] += (inp - val) ** 2\n",
    "            accuracy_samples[n] += 1\n",
    "for n in sorted(predictions):\n",
    "    accuracy[n] = (accuracy[n] / accuracy_samples[n]) ** .5\n",
    "    print(\"Predictive Error (RMS)\", n, \"steps ahead:\", accuracy[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show info about the anomaly (mean & std)\n",
    "print(\"Anomaly Mean\", np.mean(anomaly))\n",
    "print(\"Anomaly Std \", np.std(anomaly))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(2,1,1)\n",
    "plt.title(\"Predictions\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Power Consumption\")\n",
    "plt.plot(np.arange(len(inputs)), inputs, 'red',\n",
    "         np.arange(len(inputs)), predictions[1], 'blue',\n",
    "         np.arange(len(inputs)), predictions[5], 'green',)\n",
    "plt.legend(labels=('Input', '1 Step Prediction, Shifted 1 step', '5 Step Prediction, Shifted 5 steps'))\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "plt.title(\"Anomaly Score\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Power Consumption\")\n",
    "inputs = np.array(inputs) / max(inputs)\n",
    "plt.plot(np.arange(len(inputs)), inputs, 'red',\n",
    "         np.arange(len(inputs)), anomaly, 'blue',)\n",
    "plt.legend(labels=('Input', 'Anomaly Score'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "47.2727px",
    "left": "925.735px",
    "top": "51.6051px",
    "width": "160.625px"
   },
   "toc_section_display": false,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
