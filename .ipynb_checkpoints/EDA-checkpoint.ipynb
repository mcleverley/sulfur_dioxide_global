{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#imports\" data-toc-modified-id=\"imports-0.0.1\"><span class=\"toc-item-num\">0.0.1&nbsp;&nbsp;</span>imports</a></span></li><li><span><a href=\"#functions\" data-toc-modified-id=\"functions-0.0.2\"><span class=\"toc-item-num\">0.0.2&nbsp;&nbsp;</span>functions</a></span></li></ul></li></ul></li><li><span><a href=\"#EDA\" data-toc-modified-id=\"EDA-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>EDA</a></span><ul class=\"toc-item\"><li><span><a href=\"#exploring-&quot;SCIENCE_DATA&quot;\" data-toc-modified-id=\"exploring-&quot;SCIENCE_DATA&quot;-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>exploring \"SCIENCE_DATA\"</a></span><ul class=\"toc-item\"><li><span><a href=\"#right,-i-forgot-there's-a-readme-explaining-everything-quite-clearly.\" data-toc-modified-id=\"right,-i-forgot-there's-a-readme-explaining-everything-quite-clearly.-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>right, i forgot there's a readme explaining everything quite clearly.</a></span></li><li><span><a href=\"#so-how-do-i-interpret-the-320x36?\" data-toc-modified-id=\"so-how-do-i-interpret-the-320x36?-1.1.2\"><span class=\"toc-item-num\">1.1.2&nbsp;&nbsp;</span>so how do i interpret the 320x36?</a></span><ul class=\"toc-item\"><li><span><a href=\"#before-i-forget,-there's-probably-a-library-to-parse-lat/long-into-geographic-regions/cities/countries\" data-toc-modified-id=\"before-i-forget,-there's-probably-a-library-to-parse-lat/long-into-geographic-regions/cities/countries-1.1.2.1\"><span class=\"toc-item-num\">1.1.2.1&nbsp;&nbsp;</span>before i forget, there's probably a library to parse lat/long into geographic regions/cities/countries</a></span></li></ul></li></ul></li><li><span><a href=\"#Data-format-is-based-off-of-instrument-format\" data-toc-modified-id=\"Data-format-is-based-off-of-instrument-format-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Data format is based off of instrument format</a></span></li><li><span><a href=\"#filename-considerations\" data-toc-modified-id=\"filename-considerations-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>filename considerations</a></span></li><li><span><a href=\"#right,-i-think-i've-got-the-format\" data-toc-modified-id=\"right,-i-think-i've-got-the-format-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>right, i think i've got the format</a></span><ul class=\"toc-item\"><li><span><a href=\"#Matrix-matching\" data-toc-modified-id=\"Matrix-matching-1.4.1\"><span class=\"toc-item-num\">1.4.1&nbsp;&nbsp;</span>Matrix-matching</a></span></li></ul></li><li><span><a href=\"#speed-tests\" data-toc-modified-id=\"speed-tests-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>speed tests</a></span></li><li><span><a href=\"#append-time:-81s-/-.h5\" data-toc-modified-id=\"append-time:-81s-/-.h5-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>append time: 81s / .h5</a></span></li><li><span><a href=\"#list-zip-time:-78s-/-.h5\" data-toc-modified-id=\"list-zip-time:-78s-/-.h5-1.7\"><span class=\"toc-item-num\">1.7&nbsp;&nbsp;</span>list zip time: 78s / .h5</a></span></li><li><span><a href=\"#TIME-issue:\" data-toc-modified-id=\"TIME-issue:-1.8\"><span class=\"toc-item-num\">1.8&nbsp;&nbsp;</span>TIME issue:</a></span><ul class=\"toc-item\"><li><span><a href=\"#matrix-to-DataFrame-directly\" data-toc-modified-id=\"matrix-to-DataFrame-directly-1.8.1\"><span class=\"toc-item-num\">1.8.1&nbsp;&nbsp;</span>matrix to DataFrame directly</a></span></li><li><span><a href=\"#nice.-i-can-just-flatten-the-arrays-and-dataframe-it\" data-toc-modified-id=\"nice.-i-can-just-flatten-the-arrays-and-dataframe-it-1.8.2\"><span class=\"toc-item-num\">1.8.2&nbsp;&nbsp;</span>nice. i can just flatten the arrays and dataframe it</a></span></li></ul></li><li><span><a href=\"#matrix-test:-much-faster,-but-warning\" data-toc-modified-id=\"matrix-test:-much-faster,-but-warning-1.9\"><span class=\"toc-item-num\">1.9&nbsp;&nbsp;</span>matrix test: much faster, but warning</a></span></li><li><span><a href=\"#good-to-remember:-work-with-matrices-when-you-can,-it's-much-faster-than-iterating-i,j\" data-toc-modified-id=\"good-to-remember:-work-with-matrices-when-you-can,-it's-much-faster-than-iterating-i,j-1.10\"><span class=\"toc-item-num\">1.10&nbsp;&nbsp;</span>good to remember: work with matrices when you can, it's much faster than iterating i,j</a></span><ul class=\"toc-item\"><li><span><a href=\"#now-build-the-process_all_orbits()\" data-toc-modified-id=\"now-build-the-process_all_orbits()-1.10.1\"><span class=\"toc-item-num\">1.10.1&nbsp;&nbsp;</span>now build the process_all_orbits()</a></span></li></ul></li></ul></li><li><span><a href=\"#hexbin\" data-toc-modified-id=\"hexbin-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>hexbin</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def peek_dataset(ds): # only works on 2d array datasets\n",
    "    for i in range(ds.shape[0]):\n",
    "        for k in range(ds.shape[1]):\n",
    "            print(ds[i][k])\n",
    "        print('\\n')\n",
    "        \n",
    "def create_base_df():\n",
    "    d = {'lat':[0.0],'long':[0.0],'sat_lat':[0.0],'sat_long':[0.0],'sat_alt':[0.0],'time':[0.0],'sza':[0.0],\n",
    "      'pbl':[0.0],'anom':[0],'cloud':[0.0]}\n",
    "    df = pd.DataFrame(data=d)\n",
    "    return df\n",
    "    \n",
    "\n",
    "def process_orbit_old(df, h5): # pandas DF, string. adds h5's observations to dataframe\n",
    "    f = h5py.File(h5, 'r') # read file\n",
    "    geo = f['GEOLOCATION_DATA'] # h5 Groups architecture is similar to dict\n",
    "    sci = f['SCIENCE_DATA']\n",
    "    c=0\n",
    "    for i in range(320): # track positions during orbit\n",
    "        for k in range(36): # cross positions at each track position\n",
    "            print(f'adding {c}...')\n",
    "            obs = {} # 11,520 max\n",
    "            obs['lat'] = geo['Latitude'][i][k]\n",
    "            obs['long'] = geo['Longitude'][i][k]\n",
    "            obs['sat_lat'] = geo['SpacecraftLatitude'][i] # 1d so just use 1st loop\n",
    "            obs['sat_long'] = geo['SpacecraftLongitude'][i]\n",
    "            obs['time'] = geo['TimeUTC'][i]\n",
    "            obs['sza'] = geo['SolarZenithAngle'][i]\n",
    "            obs['pbl'] = sci['ColumnAmountSO2_PBL'][i][k]\n",
    "            obs['anom'] = sci['Flag_SAA'][i][k]\n",
    "            obs['cloud'] = sci['RadiativeCloudFraction'][i][k]\n",
    "            # we don't have to filter out by SZA, Anomaly etc quite yet. save that for after the data's gathered\n",
    "            df = df.append(obs, ignore_index=True)\n",
    "            c+=1\n",
    "    # end loops\n",
    "    print(f'dataframe length: {len(df)}')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"many_outs.h5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "okay, found the issue\n",
    "- for downloading multiple .h5 files, nasa reccomends:\n",
    "    - cat <url.txt> | tr -d '\\r' | xargs -n 1 curl -LJO -n -c ~/.urs_cookies -b ~/.urs_cookies\n",
    "    - but i was getting some access denied + use --output recommendations. didn't work\n",
    "        - so i tried >> piping it to a file. i could have named it \"data.h5\", maybe? can you store h5s in other h5s?\n",
    "        - i named it 'data.iso' on someone's SO solution\n",
    "        - this went poorly. the data downloaded, but when trying to open it, it seemed super corrupted, even with specific encoding that allowed it to open\n",
    "    - i went back to NASA's downloading instructions and tested the curl cmd with a smaller subset of .h5 files, which worked (i think i had some typos in the cmdline, or i was formatting incorrectly) so i applied it to the original 2012-2020 46gb of data. currently downloading all the .h5s to my home directory, so i'll just have to move them en masse and loop-open them into a dataframe in the project folder (in 5 hours when it downloads)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# for filename in os.listdir(os.getcwd()):\n",
    "#    with open(os.path.join(os.cwd(), filename), 'r') as f: # open in readonly mode\n",
    "#       # do your stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys: <KeysViewHDF5 ['ANCILLARY_DATA', 'GEOLOCATION_DATA', 'SCIENCE_DATA', 'nTimes', 'nWavel2', 'nWavel3', 'nXtrack']>\n"
     ]
    }
   ],
   "source": [
    "lst = []\n",
    "for filename in os.listdir(os.getcwd()):\n",
    "    if filename.endswith('.h5'):\n",
    "        with h5py.File(filename, 'r') as f:\n",
    "            print('keys: %s' %f.keys())\n",
    "            a_group_key = list(f.keys())[0]\n",
    "            data = list(f[a_group_key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "f = h5py.File('one.h5', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KeysViewHDF5 ['ANCILLARY_DATA', 'GEOLOCATION_DATA', 'SCIENCE_DATA', 'nTimes', 'nWavel2', 'nWavel3', 'nXtrack']>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANCILLARY_DATA <class 'h5py._hl.group.Group'> \n",
      "\n",
      "GEOLOCATION_DATA <class 'h5py._hl.group.Group'> \n",
      "\n",
      "SCIENCE_DATA <class 'h5py._hl.group.Group'> \n",
      "\n",
      "nTimes <class 'h5py._hl.dataset.Dataset'> \n",
      "\n",
      "nWavel2 <class 'h5py._hl.dataset.Dataset'> \n",
      "\n",
      "nWavel3 <class 'h5py._hl.dataset.Dataset'> \n",
      "\n",
      "nXtrack <class 'h5py._hl.dataset.Dataset'> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k in f.keys():\n",
    "    print(k, type(f[k]), '\\n') # remember to use the key on the dict obj to actually get the type of value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- so the root h5py object is a dictionary... of sorts.\n",
    "    - we've got Groups, which are like folders, and Datasets, which are like arrays (but different)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KeysViewHDF5 ['Latitude', 'Longitude', 'SolarAzimuthAngle', 'SolarZenithAngle', 'SpacecraftAltitude', 'SpacecraftLatitude', 'SpacecraftLongitude', 'TimeTAI93', 'TimeUTC', 'ViewingAzimuthAngle', 'ViewingZenithAngle']>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f['GEOLOCATION_DATA'].keys() # looks like location of Suomi satellite at moment of reading\n",
    "# we're gonna want Latitude and Longitude to identify 'where' the reading is.\n",
    "# maybe TimeUTC as well, that could save a lot of re-coding work for broad regional targeting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KeysViewHDF5 ['CloudPressure', 'TerrainPressure']>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f['ANCILLARY_DATA'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KeysViewHDF5 ['CloudFraction', 'ColumnAmountO3', 'ColumnAmountSO2_PBL', 'ColumnAmountSO2_STL', 'ColumnAmountSO2_TRL', 'ColumnAmountSO2_TRM', 'ColumnAmountSO2_TRU', 'FittingWindow_STL', 'FittingWindow_TRL', 'FittingWindow_TRM', 'FittingWindow_TRU', 'Flag_SAA', 'Flag_SO2', 'RadiativeCloudFraction', 'Reflectivity331', 'SLER', 'UVAerosolIndex', 'Wavelengths_SLER', 'dNdR', 'nPrincipalComponents']>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f['SCIENCE_DATA'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from the NASA dataset description:\n",
    "\n",
    "The goal of this dataset is to create and archive a Level 2 Sulfur Dioxide (SO2) Earth Science Data Record (ESDR) from backscatter Ultraviolet (BUV) measurements from the Ozone Mapping and Profiler Suite (OMPS) Nadir Mapper (NM) flown on the NASA/NOAA Suomi National Polar-orbiting Partnership (S-NPP) satellite since 2011. We apply the NASA Goddard Space Flight Center (GSFC) principal component analysis (PCA) satellite trace gas retrieval algorithm, to obtain the best measurement-based ESDR of volcanic and anthropogenic SO2 loadings and emissions, which is fully consistent with our SO2 ESDR (OMSO2) from the Ozone Monitoring Instrument (OMI) flown on the NASA Earth Observing System (EOS) Aura satellite since 2004. \n",
    "\n",
    "The SO2 ESDR released here at the Goddard Earth Science (GES) Data and Information Data Center (DISC), OMPS_NPP_NMSO2_PCA_L2, is a stand-alone dataset, but will also be part of the NASA's Making Earth System Data Records for Use in Research Environments (MEaSUREs) program. It covers the entire period of the S-NPP/OMPS mission since launch in October 2011, and forward data processing is ongoing. OMPS_NPP_NMSO2_PCA_L2 is a Level 2 orbital swath product, which will be used to study SO2 pollutants emitted from large point sources such as coal-fired power plants and smelters as well as to monitor volcanic SO2 emissions.\n",
    "\n",
    "Sulfur Dioxide (SO2) is a short-lived gas primarily produced by volcanoes, power plants, refineries, metal smelting and burning of fossil fuels. Where SO2 remains near the Earth's surface, it is toxic, causes acid rain, and degrades air quality. Where SO2 is lofted into the free troposphere, it forms aerosols that can alter cloud reflectivity and precipitation. In the stratosphere, volcanic SO2 forms sulfate aerosols that can result in climate change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- translation:\n",
    "    - here's SO2 levels by latitude, date & many other variables\n",
    "        - derived from applying PCA algos to Ozone mapping data, which was measured from backscatter ultraviolet readings\n",
    "                - basically, NASA looks at backscattered UV off of the atmosphere - off of ozone, but can effectively derive SO2 from the ozone reading using PCA\n",
    "        - seems convoluted, but it's consistent with their last SO2 reading system from 2004, and it's probably the best estimate the world's got right now (unless anyone would like to step up and accurately measure trace gas concentrations globally every day. seems bloody hard in my opinion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## exploring \"SCIENCE_DATA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KeysViewHDF5 ['CloudFraction', 'ColumnAmountO3', 'ColumnAmountSO2_PBL', 'ColumnAmountSO2_STL', 'ColumnAmountSO2_TRL', 'ColumnAmountSO2_TRM', 'ColumnAmountSO2_TRU', 'FittingWindow_STL', 'FittingWindow_TRL', 'FittingWindow_TRM', 'FittingWindow_TRU', 'Flag_SAA', 'Flag_SO2', 'RadiativeCloudFraction', 'Reflectivity331', 'SLER', 'UVAerosolIndex', 'Wavelengths_SLER', 'dNdR', 'nPrincipalComponents']>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f['SCIENCE_DATA'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"ColumnAmountSO2_PBL\": shape (320, 36), type \"<f4\">"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f['SCIENCE_DATA']['ColumnAmountSO2_PBL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbl = f['SCIENCE_DATA']['ColumnAmountSO2_PBL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'geo' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-a3b1c54d52b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgeo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'geo' is not defined"
     ]
    }
   ],
   "source": [
    "geo.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo['TimeUTC'][310]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# peek_dataset(pbl) # a 320x36 matrix. now to figure out why there's 11520 readings in this dataset, which is one .h5 file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in f['SCIENCE_DATA'].keys(): # are they all 320x36? probably the SO2 at least\n",
    "#     print(k, type(k))\n",
    "    if 'SO2' in k: # k is a string\n",
    "        print(k, f['SCIENCE_DATA'][k].shape) # yes, the SO2 readings are matched in size.\n",
    "# probably different instruments or estimation methods. perhaps we could aggregate them later on, but they're all going in the dataframe for now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### right, i forgot there's a readme explaining everything quite clearly.\n",
    "- units\n",
    "    - 5 estimates (principal components), related to various geophysical processes (ozone absorption, rotational Raman scattering) and instrument measurement details (wavelength shift, dark current)\n",
    "    - PCs used in SO2 spectral fitting to reduce interferences\n",
    "        - so we've got five estimates of total \"SO2 Vertical Column Density\" in Dobson Units (1DU = 2.69 * 10^16 molecules/cm^2)\n",
    "            - note the \"cm^2\". these are flat \"slices\" of a column? no\n",
    "            - each of the 5 estimates corresponds to a different assumed SO2 cloud height, or Center of Mass Altitude (CMA)\n",
    "            - all 5 VCD values represent the estimated total SO2 burden within entire atmospheric column:\n",
    "                - should not be interpreted as partial column amounts within different layers/parts of atmosphere\n",
    "                - they want users to select 1 or interpolate between two estimates that are most representative of conditions for a particular case of interest\n",
    "- so each of the 5 estimates measures with different heights/sensitivities, and therefore are 'tuned' to measure different... types of SO2 emission.\n",
    "    - anthropogenic (coal factories etc) SO2 emissions will look different, and be measured differently from volcanic eruptions\n",
    "    - that sort of thing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- PBL: Planetary Boundary Layer\n",
    "    - assumes that SO2 is predominantly in lowest 1km of atmosphere\n",
    "    - use for studies on near-surface pollution\n",
    "        - for each OMPS orbit:\n",
    "            - process 36 rows (cross-track positions) one at a time\n",
    "            - use PCA to extract PCs for spectral range 310.5-340nm from sun-normalized BUV radiance spectra\n",
    "            - PCs are ranked in descending order according to spectral variance that they explain\n",
    "    - due to jacobian simplication, they recommend only using snow/ice free pixels with small radiative cloud fraction\n",
    "- the other 4 column amounts seem to be volcanic\n",
    "### so how do i interpret the 320x36?\n",
    "- wait, they're talking about pixels\n",
    "    - 320x36 is the \"resolution\" of the measurement\n",
    "    \n",
    "#### before i forget, there's probably a library to parse lat/long into geographic regions/cities/countries\n",
    "\n",
    "- it's an \"orbital-track\" product about the SNPP satellite.\n",
    "    - polar sun-synchronous orbit\n",
    "## Data format is based off of instrument format\n",
    "- OMPS-NM has 110° FoV, cross-track swath ~= 2800km\n",
    "    - global coverage,14-15 orbits/day\n",
    "    - \"nominal spatial resolution\" is 50km x 50km at nadir in nominal observation mode\n",
    "        - there's a higher res mode recorded once a week, but that's not in this dataset\n",
    "- for each OMPS orbit, process 36 rows (cross-track positions) use PCA to extract PCs for spectral range from BUV\n",
    "    - so, 320 PCs per position while crossing orbital track?\n",
    "        - what does the lat/long data look like? that might solve this question\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat = f['GEOLOCATION_DATA']['Latitude']\n",
    "long = f['GEOLOCATION_DATA']['Longitude']\n",
    "print(lat.shape, long.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- so in one .h5's 36x320:\n",
    "    - began at -29, -152: about 1/3 of the way from NZ to Ecuador\n",
    "    - ended at 67, 16: northern middle Norway\n",
    "        - but those aren't the extreme bounds for lat or long.\n",
    "            - without plotting it out, my intuition says it's an oscillating path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# peek_dataset(lat)\n",
    "# peek_dataset(long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- now the 50km spatial resolution makes sense.\n",
    "    - we've got 11520 'pixels' or 'granules' (are granules the .h5?)\n",
    "    - each pixel has an SO2 PBL reading, as well as a lat/long\n",
    "        - so the corresponding [x],[y] position in the matrix will match a geolocation to its PBL\n",
    "- warnings from the readme:\n",
    "    - all pixels:\n",
    "        - with large solar zenith angle (SZA>70)\n",
    "        - near edge of swath (rows 1-2, 35-36)\n",
    "        - affected by South Atlantic Anomaly (flag_SAA=1)\n",
    "            - should be excluded\n",
    "    - PBL SO2:\n",
    "        - only use:\n",
    "            - snow/ice free pixels\n",
    "            - with RadiativeCloudFraction (<0.3)\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## filename considerations\n",
    "- each .h5 file contains:\n",
    "    - date/time at start of orbit\n",
    "    - orbit number\n",
    "        - so each .h5 is an orbit. question answered\n",
    "- what about time? that should help me figure out \"unique positions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo = f['GEOLOCATION_DATA']\n",
    "time = geo['TimeUTC']\n",
    "time.shape # ah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lat = geo['Latitude']\n",
    "lat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat[0][5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- so we've got 320 times per orbit, but 320x36 pixels and 320x36 lat/long coords\n",
    "- that means at 320 times throughout the day, the satellite gathers 36 pixels, and each one has a unique geolocation...?\n",
    "    - the 36 is \"cross-track positions\"\n",
    "        - is it a vertical \"36 slices of column underneath spacecraft\"?\n",
    "            - no\n",
    "            - if that were the case, \n",
    "                - there wouldn't be a separate lat/long for spacecraft as well\n",
    "                    - the \"spacecraft lat/long\" is (320) while \"lat/long\" is (320,36)\n",
    "                    - lines up with 320 timestamps\n",
    "    - how to interpret \"cross-track positions\"\n",
    "        - 36 positions that cross the track (orbital track path over earth's surface)\n",
    "        - so for each 320 position+timestamp along track, there's 36 \"positions\" that cross the track\n",
    "        - i'm assuming it does a left/right sweep below in its 110° fov while continuing \"forwards\"\n",
    "            - this makes the most sense. spacecraft geocoords are 1d, geocoords are 2d, so the 2nd dimension from line to plane has to be added somehow\n",
    "            - i could be stupendously wrong, but let's worry about that later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## right, i think i've got the format\n",
    "- each .h5 file is an orbit, with 320 track positions (geocoords+timestamp)\n",
    "    - each track position has 36 cross-track positions, so we get 320x36 matrix for readings+GPS\n",
    "    - each track/crosstrack combo is a 50km x 50km space of earth (i think...)\n",
    "        - the lat/long is decimals, but that can be converted to degrees if needed\n",
    "- for each .h5 file, we've got 11,520 PBL readings, but not all of them will be usable\n",
    "    - ice/snow, large solar zenith angle, south atlantic magnetic interference\n",
    "- and we've got something like... 40,000 or 50,000 h5s, maybe more\n",
    "### Matrix-matching\n",
    "- lat[5][6], long[5][6] and PBT[5][6] will give us the location and reading for the 5th position, 6th cross position\n",
    "    - so we can just write functions to neatly iterate through each h5's file tree and add each 'observation' to a big dataframe\n",
    "    - it's going to be big\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## speed tests\n",
    "- append or concat? 11,520 matrix elements, 36,000ish orbits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "start = time.time()\n",
    "print(\"hello\")\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_base_df():\n",
    "    d = {'lat':[0.0],'long':[0.0],'sat_lat':[0.0],'sat_long':[0.0],'time':[0.0],'sza':[0.0],\n",
    "      'pbl':[0.0],'anom':[0],'cloud':[0.0]}\n",
    "    df = pd.DataFrame(data=d)\n",
    "    return df\n",
    "\n",
    "def process_orbit_app(df, h5): # pandas DF, string. adds h5's observations to dataframe\n",
    "    start = time.time()\n",
    "    f = h5py.File(h5, 'r') # read file\n",
    "    geo = f['GEOLOCATION_DATA'] # h5 Groups architecture is similar to dict\n",
    "    sci = f['SCIENCE_DATA']\n",
    "    c=0\n",
    "    for i in range(320): # track positions during orbit\n",
    "        for k in range(36): # cross positions at each track position\n",
    "            print(f'adding {c}...')\n",
    "            obs = {} # 11,520 max\n",
    "            obs['lat'] = geo['Latitude'][i][k]\n",
    "            obs['long'] = geo['Longitude'][i][k]\n",
    "            obs['sat_lat'] = geo['SpacecraftLatitude'][i] # 1d so just use 1st loop\n",
    "            obs['sat_long'] = geo['SpacecraftLongitude'][i]\n",
    "            obs['time'] = geo['TimeUTC'][i]\n",
    "            obs['sza'] = geo['SolarZenithAngle'][i]\n",
    "            obs['pbl'] = sci['ColumnAmountSO2_PBL'][i][k]\n",
    "            obs['anom'] = sci['Flag_SAA'][i][k]\n",
    "            obs['cloud'] = sci['RadiativeCloudFraction'][i][k]\n",
    "            # we don't have to filter out by SZA, Anomaly etc quite yet. save that for after the data's gathered\n",
    "            df = df.append(obs, ignore_index=True)\n",
    "            c+=1\n",
    "    # end loops\n",
    "    print(f'dataframe length: {len(df)}')\n",
    "    end = time.time()\n",
    "    print(f'TIME .append()  :  {end - start}')\n",
    "    return df\n",
    "\n",
    "def process_orbit_list(df, h5): # pandas DF, string. adds h5's observations to dataframe\n",
    "#     start = time.time()\n",
    "    f = h5py.File(h5, 'r') # read file\n",
    "    geo = f['GEOLOCATION_DATA'] # h5 Groups architecture is similar to dict\n",
    "    sci = f['SCIENCE_DATA']\n",
    "    c=0\n",
    "    lat, long, sat_lat, sat_long, time, sza, pbl, anom, cloud = ([] for i in range(9)) # create 9 empty lists\n",
    "    for i in range(320): # track positions during orbit\n",
    "        for k in range(36): # cross positions at each track position\n",
    "            print(f'adding {c}...') # add to each list\n",
    "            lat.append(geo['Latitude'][i][k])\n",
    "            long.append(geo['Longitude'][i][k])\n",
    "            sat_lat.append(geo['SpacecraftLatitude'][i]) # 1d so just use 1st loop\n",
    "            sat_long.append(geo['SpacecraftLongitude'][i])\n",
    "            time.append(geo['TimeUTC'][i])\n",
    "            sza.append(geo['SolarZenithAngle'][i])\n",
    "            pbl.append(sci['ColumnAmountSO2_PBL'][i][k])\n",
    "            anom.append(sci['Flag_SAA'][i][k])\n",
    "            cloud.append(sci['RadiativeCloudFraction'][i][k])\n",
    "            # we don't have to filter out by SZA, Anomaly etc quite yet. save that for after the data's gathered\n",
    "            c+=1\n",
    "    # end loops\n",
    "    df = pd.DataFrame(list(zip(lat, long, sat_lat, sat_long, time, sza, pbl, anom, cloud)),\n",
    "                     columns=[\"lat\", \"long\", \"sat_lat\", \"sat_long\", \"time\", \"sza\", \"pbl\", \"anom\", \"cloud\"])\n",
    "    print(f'dataframe length: {len(df)}')\n",
    "#     end = time.time()\n",
    "#     print(f'TIME .append()  :  {end - start}')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_df(cols, ind):\n",
    "    \"\"\"Quickly make a DataFrame\"\"\"\n",
    "    data = {c: [str(c) + str(i) for i in ind]\n",
    "            for c in cols}\n",
    "    return pd.DataFrame(data, ind)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = make_df('AB', [0, 1])\n",
    "df4 = make_df('CD', [0, 1])\n",
    "display('df3', 'df4', \"pd.concat([df3, df4], axis='col')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([df3, df4], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of strings \n",
    "lst = ['Geeks', 'For', 'Geeks', 'is', 'portal', 'for', 'Geeks'] \n",
    "  \n",
    "# list of int \n",
    "lst2 = [11, 22, 33, 44, 55, 66, 77] \n",
    "lst3 = ['a','b','c','d','e','f','g']\n",
    "  \n",
    "# Calling DataFrame constructor after zipping \n",
    "# both lists, with columns specified \n",
    "df = pd.DataFrame(list(zip(lst, lst2, lst3)), \n",
    "               columns =['Name', 'val', 'chr']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filename = 'one.h5'\n",
    "df = create_base_df()\n",
    "start = time.time()\n",
    "df = process_orbit_app(df, filename)\n",
    "end = time.time()\n",
    "print(f'TIME .append()  :  {end - start}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## append time: 81s / .h5\n",
    "## list zip time: 78s / .h5\n",
    "- closer than i thought. i thought i read .concat() was faster? but then again i'm not using concat in the end - that combines dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TIME issue:\n",
    "- 80 seconds per h5 for 38k h5s gives us 844 hours of runtime for the function.\n",
    "    - that is absolutely uncool\n",
    "- is it an unavoidable issue with the amount of data in terms of functional calls?\n",
    "    - i'm dealing with h5.Datasets, which are basically arrays\n",
    "        - i have a feeling there's a faster way to array->dataframe than nested loop appending\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sci.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### matrix to DataFrame directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = sci['ColumnAmountSO2_PBL']\n",
    "mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = pd.DataFrame(mat) # 320 times a day, 36 cross-track positions are measured\n",
    "m # could we just compress the 2d matrix to 1d, keeping track every 36?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- i have 9 matrices, each 320x36 for 11520 observations\n",
    "    - or rather, ~4 are just 320 ( but i could extend them into 320x36 )\n",
    "    - how can i make a dataframe shaped (11520, 9)\n",
    "    - pretty sure pd.concat() will work here\n",
    "- i think flattening it out to 1d could simplify things greatly\n",
    "    - is there any reason to keep 2d shape? no...?\n",
    "        - from a data structure standpoint, the 2d shape tells us:\n",
    "            - \"which observations belong to the same track-position?\"\n",
    "        - so if we flatten all matrices equally (and for the 1d, just multiply by 36)\n",
    "            - then the structure should be solved / positions preserved?\n",
    "        - then if i have 9 (11520) matrices, i basically have a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = mat.value # convert to nparray\n",
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbl = matrix.ravel() # looking at the 1st and last values, it seems like order is preserved (why wouldn't it be?)\n",
    "pbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbl.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sci.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clouds = sci['RadiativeCloudFraction'].value.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clouds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clouds_lst = list(clouds)\n",
    "pbl_lst = list(pbl)\n",
    "df = pd.DataFrame(list(zip(clouds_lst, pbl_lst)), columns=['clouds', 'pbl'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.copy() # deep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape, df2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d3 = pd.concat([df,df2])\n",
    "d3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nice. i can just flatten the arrays and dataframe it\n",
    "- this problem was tough until i let go of the notion that i need the 320x36 shape\n",
    "    - when you're stuck, reconsidering your assumptions/accepted 'requirements' can help\n",
    "- so the new process_h5_matrix() function:\n",
    "    - open h5\n",
    "    - assign datasets to vars\n",
    "    - if 2d: flatten to 1d\n",
    "    - if 1d: multiply length by 36\n",
    "    - list & zip into DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = create_base_df()\n",
    "start = time.time()\n",
    "df = process_orbit(df, filename)\n",
    "end = time.time()\n",
    "print(f'time:  {end-start}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## matrix test: much faster, but warning\n",
    "time:  0.01427316665649414\n",
    "/Users/mark/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:26: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
    "of pandas will change to not sort by default.\n",
    "\n",
    "To accept the future behavior, pass 'sort=False'.\n",
    "\n",
    "To retain the current behavior and silence the warning, pass 'sort=True'.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## good to remember: work with matrices when you can, it's much faster than iterating i,j "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### now build the process_all_orbits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = f'{1}s'\n",
    "st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_base_df():\n",
    "    d = {'lat':[0.0],'long':[0.0],'sat_lat':[0.0],'sat_long':[0.0],'sat_alt':[0.0],'time':[0.0],'sza':[0.0],\n",
    "      'pbl':[0.0],'anom':[0],'cloud':[0.0]}\n",
    "    df = pd.DataFrame(data=d)\n",
    "    return df\n",
    "    \n",
    "\n",
    "def process_orbit(h5): # pandas DF, filename string. adds h5's observations to dataframe\n",
    "    f = h5py.File(h5, 'r') # read file\n",
    "    geo = f['GEOLOCATION_DATA'] # h5 Groups architecture is similar to dict\n",
    "    sci = f['SCIENCE_DATA']\n",
    "    # hdf.Datasets -> np.array -> list\n",
    "    lat = list(geo['Latitude'].value.ravel())\n",
    "    long = list(geo['Longitude'].value.ravel()) # is there a less verbose way to do this?\n",
    "    sat_lat = list(geo['SpacecraftLatitude'].value.ravel())*36 # extend the 1d arrays\n",
    "    sat_long = list(geo['SpacecraftLongitude'].value.ravel())*36 # don't forget to ravel like i did\n",
    "    sat_alt = list(geo['SpacecraftAltitude'].value.ravel())*36\n",
    "    time = list(geo['TimeUTC'].value.ravel())*36 # 36 measurements per position means one \"time\" value for every consecutive 36 measurements\n",
    "    sza = list(geo['SolarZenithAngle'].value.ravel())\n",
    "    pbl = list(sci['ColumnAmountSO2_PBL'].value.ravel())\n",
    "    anom = list(sci['Flag_SAA'].value.ravel())\n",
    "    # combine lists into df\n",
    "    new = pd.DataFrame(list(zip(lat, long, sat_lat, sat_long, sat_alt, time, sza, pbl, anom)),\n",
    "                       columns=[\"lat\", \"long\", \"sat_lat\", \"sat_long\", 'sat_alt', \"time\", \"sza\", \"pbl\", \"anom\"])\n",
    "    # we don't have to filter out by SZA, Anomaly etc quite yet. save that for after the data's gathered\n",
    "#     df = pd.concat([df, new]) # merge 11,520 into existing df\n",
    "\n",
    "#     print(len(df))\n",
    "    return new # returns new df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "# def process_all_h5(df): # loads all h5s into a dataframe. will save locally every ~500 files\n",
    "#     mypath = os.getcwd() + \"/data/\" # get all files in data folder\n",
    "#     files = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "#     for f in tqdm(files): # h5py is kind of picky with absolute filenames & root folder files\n",
    "#         path = mypath+f\n",
    "#         df = process_orbit(df, path) # ah, that did the trick\n",
    "        # enumerate instead of tqdm to allow saving every 100 .h5s\n",
    "    # more stable in case of crash. could also write this to upload to mysql & keep track of processed/unprocessed files\n",
    "def process_all_h5(df): # loads all h5s into a dataframe. will save locally every ~500 files\n",
    "    mypath = os.getcwd() + \"/data/\" # get all files in data folder\n",
    "    files = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "    for c,f in enumerate(files): # h5py is kind of picky with absolute filenames & root folder files\n",
    "        print(c)\n",
    "        path = mypath+f\n",
    "        new = process_orbit(path)\n",
    "        df = pd.concat([df, new])\n",
    "#         if c/100 == 0:\n",
    "#             st = f'increment_{c}'\n",
    "#             df.to_csv(st+'.csv')\n",
    "        print(f'LENGTH:  {len(df)}')\n",
    "        df.to_csv('increm.csv')\n",
    "    return df\n",
    "\n",
    "df = create_base_df()\n",
    "start = time.time()\n",
    "df = process_all_h5(df)\n",
    "df.to_csv('all_data.csv')\n",
    "end = time.time()\n",
    "print(f'time:   {end-start}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2/100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.read_csv('increm.csv')\n",
    "len(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = []\n",
    "for filename in os.listdir(os.getcwd()):\n",
    "    if filename.endswith('.h5'):\n",
    "        with h5py.File(filename, 'r') as f:\n",
    "            print('keys: %s' %f.keys())\n",
    "            a_group_key = list(f.keys())[0]\n",
    "            data = list(f[a_group_key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo = f['GEOLOCATION_DATA'] # h5 Groups architecture is similar to dict\n",
    "sci = f['SCIENCE_DATA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KeysViewHDF5 ['CloudFraction', 'ColumnAmountO3', 'ColumnAmountSO2_PBL', 'ColumnAmountSO2_STL', 'ColumnAmountSO2_TRL', 'ColumnAmountSO2_TRM', 'ColumnAmountSO2_TRU', 'FittingWindow_STL', 'FittingWindow_TRL', 'FittingWindow_TRM', 'FittingWindow_TRU', 'Flag_SAA', 'Flag_SO2', 'RadiativeCloudFraction', 'Reflectivity331', 'SLER', 'UVAerosolIndex', 'Wavelengths_SLER', 'dNdR', 'nPrincipalComponents']>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sci.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CloudFraction <class 'numpy.float32'> 0.21159439\n",
      "ColumnAmountO3 <class 'numpy.float32'> 281.70758\n",
      "ColumnAmountSO2_PBL <class 'numpy.float32'> -0.40295467\n",
      "ColumnAmountSO2_STL <class 'numpy.float32'> -0.060004227\n",
      "ColumnAmountSO2_TRL <class 'numpy.float32'> -0.17981637\n",
      "ColumnAmountSO2_TRM <class 'numpy.float32'> -0.089290515\n",
      "ColumnAmountSO2_TRU <class 'numpy.float32'> -0.06284826\n",
      "FittingWindow_STL <class 'numpy.ndarray'> [313. 340.]\n",
      "FittingWindow_TRL <class 'numpy.ndarray'> [313. 340.]\n",
      "FittingWindow_TRM <class 'numpy.ndarray'> [313. 340.]\n",
      "FittingWindow_TRU <class 'numpy.ndarray'> [313. 340.]\n",
      "Flag_SAA <class 'numpy.int32'> 0\n",
      "Flag_SO2 <class 'numpy.int32'> 0\n",
      "RadiativeCloudFraction <class 'numpy.float32'> 0.29958948\n",
      "Reflectivity331 <class 'numpy.float32'> 0.28990737\n",
      "SLER <class 'numpy.ndarray'> [0.29474375 0.30017498 0.3100103 ]\n",
      "UVAerosolIndex <class 'numpy.float32'> 0.54779524\n",
      "Wavelengths_SLER <class 'numpy.ndarray'> [342.66742 353.91214 366.8322 ]\n",
      "dNdR <class 'numpy.ndarray'> [-35.882015 -40.851948 -46.43574 ]\n",
      "nPrincipalComponents <class 'numpy.int32'> 15\n"
     ]
    }
   ],
   "source": [
    "for k in sci.keys():\n",
    "    print(k, type(sci[k][0][0]), sci[k][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latitude <class 'numpy.float32'> -29.434868\n",
      "Longitude <class 'numpy.float32'> -152.67432\n",
      "SolarAzimuthAngle <class 'numpy.float32'> -14.087847\n",
      "SolarZenithAngle <class 'numpy.float32'> 53.389477\n",
      "1d SpacecraftAltitude <class 'numpy.float32'> 836490.5\n",
      "1d SpacecraftLatitude <class 'numpy.float32'> -25.943134\n",
      "1d SpacecraftLongitude <class 'numpy.float32'> -139.42757\n",
      "1d TimeTAI93 <class 'numpy.float64'> 613090702.888068\n",
      "TimeUTC <class 'int'> 50\n",
      "ViewingAzimuthAngle <class 'numpy.float32'> 76.658226\n",
      "ViewingZenithAngle <class 'numpy.float32'> 66.20959\n"
     ]
    }
   ],
   "source": [
    "for k in geo.keys():\n",
    "    try:\n",
    "        print(k, type(geo[k][0][0]), geo[k][0][0])\n",
    "    except:\n",
    "        print('1d', k, type(geo[k][0]), geo[k][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = geo['TimeUTC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'2012-06-05T22:58:15.888068Z'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.bytes_"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(time[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "tim = time[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col1</th>\n",
       "      <th>col2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>b'2012-06-05T22:58:15.888068Z'</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>b'2012-06-05T22:58:15.888068Z'</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             col1  col2\n",
       "0  b'2012-06-05T22:58:15.888068Z'     3\n",
       "1  b'2012-06-05T22:58:15.888068Z'     4"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {'col1': [tim, tim], 'col2': [3, 4]}\n",
    "df = pd.DataFrame(data=d)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "col1    object\n",
       "col2     int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['col1'] = df['col1'].apply(lambda t: t.tostring())\n",
    "df['col1'] = df['col1'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"b'2012-06-05T22:58:15.888068Z'\""
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['col1'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df['col1'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'2012-06-05T22:58:15.888068Z'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st = time[0].tostring() # bytes to str\n",
    "st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2012-06-05 22:58:15'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st = \"b'2012-06-05T22:58:15.888068Z'\" # change NASA format to mysql DATETIME\n",
    "timest = st[2:12]+' '+st[13:21]\n",
    "timest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hexbin\n",
    "hexbin code was somewhere in this notebook? let's find it again for GIS testing elsewhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
