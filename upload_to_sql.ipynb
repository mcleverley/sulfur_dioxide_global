{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#imports\" data-toc-modified-id=\"imports-0.0.1\"><span class=\"toc-item-num\">0.0.1&nbsp;&nbsp;</span>imports</a></span></li></ul></li><li><span><a href=\"#plan:\" data-toc-modified-id=\"plan:-0.1\"><span class=\"toc-item-num\">0.1&nbsp;&nbsp;</span>plan:</a></span><ul class=\"toc-item\"><li><span><a href=\"#updates-to-original-extraction-process:\" data-toc-modified-id=\"updates-to-original-extraction-process:-0.1.1\"><span class=\"toc-item-num\">0.1.1&nbsp;&nbsp;</span>updates to original extraction process:</a></span></li></ul></li></ul></li><li><span><a href=\"#methods\" data-toc-modified-id=\"methods-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>methods</a></span><ul class=\"toc-item\"><li><span><a href=\"#engine-method\" data-toc-modified-id=\"engine-method-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>engine method</a></span></li></ul></li><li><span><a href=\"#upload-complete!-(17-hours-later)\" data-toc-modified-id=\"upload-complete!-(17-hours-later)-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>upload complete! (17 hours later)</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install mysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mysql.connector\n",
    "from tqdm import tqdm\n",
    "import h5py\n",
    "from sqlalchemy import types, create_engine \n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import config\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# port 3306\n",
    "# 127.0.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('increment_0.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the \"parse all at once method\" was overwriting to the same file, and running into local memory issues. more difficult to run at length if interruptions necessary as well.\n",
    "\n",
    "## plan:\n",
    "- get all filenames in data folder\n",
    "    - process each file name to a dataframe\n",
    "        - add the dataframe to mySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine('mysql+mysqldb://...', pool_recycle=3600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatiron.clqqz5nrghvl.us-east-1.rds.amazonaws.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3306"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pymysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = config.user\n",
    "pw = config.pw\n",
    "# create engine\n",
    "connst = f'mysql+pymysql://{user}:{pw}@flatiron.clqqz5nrghvl.us-east-1.rds.amazonaws.com/so2'\n",
    "engine = create_engine(connst, echo=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'name' : ['User 1', 'User 2', 'User 3']})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'lat':[1.5],'long':[0.0],'sat_lat':[0.0],'sat_long':[0.0],'sat_alt':[0.0],'time':[0.0],'sza':[0.0],\n",
    "      'pbl':[0.0],'anom':[0],'cloud':[0.0]}\n",
    "df = pd.DataFrame(data=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_sql('so2', con=engine, if_exists='append')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.execute(\"SELECT * FROM users\").fetchall()\n",
    "[(0, 'User 1'), (1, 'User 2'), (2, 'User 3')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy.orm import sessionmaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.dispose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(connst, echo=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Session = sessionmaker(bind=engine) # create session class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = Session()\n",
    "# create session obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = engine.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'lat':[0.0],'long':[0.0],'sat_lat':[0.0],'sat_long':[0.0],'sat_alt':[0.0],'time':[0.0],'sza':[0.0],\n",
    "      'pbl':[0.0],'anom':[0],'cloud':[0.0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i think you don't have to specify with float....\n",
    "create_so2_table = '''\n",
    "CREATE TABLE so2 (\n",
    "id INT(9) UNSIGNED AUTO_INCREMENT PRIMARY KEY,\n",
    "pbl FLOAT(9,6) NOT NULL,\n",
    "latitude FLOAT(9,6) NOT NULL,\n",
    "longitude FLOAT(9,6) NOT NULL,\n",
    "time VARCHAR(35) NOT NULL,\n",
    "cloud FLOAT(9,6),\n",
    "sza FLOAT(9,6),\n",
    "anom INT(1),\n",
    "volc_so2 INT(1),\n",
    "sat_lat FLOAT(9,6) NOT NULL,\n",
    "sat_long FLOAT(9,6) NOT NULL,\n",
    "year INT(4),\n",
    "month INT(2)\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# con.execute(create_so2_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# match this format with the dataframe\n",
    "create_so2_table = '''\n",
    "CREATE TABLE so2 (\n",
    "id INT(9) UNSIGNED AUTO_INCREMENT PRIMARY KEY,\n",
    "pbl FLOAT(9,6) NOT NULL,\n",
    "latitude FLOAT(9,6) NOT NULL,\n",
    "longitude FLOAT(9,6) NOT NULL,\n",
    "time VARCHAR(35) NOT NULL,\n",
    "cloud FLOAT(9,6),\n",
    "sza FLOAT(9,6),\n",
    "anom INT(1),\n",
    "volc_so2 INT(1),\n",
    "sat_lat FLOAT(9,6) NOT NULL,\n",
    "sat_long FLOAT(9,6) NOT NULL,\n",
    "year INT(4),\n",
    "month INT(2)\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### updates to original extraction process:\n",
    "- also extracting 'Flag_SO2' which marks volcanic activity. useful\n",
    "- 'TimeUTC' bytearray -> string\n",
    "    - DATETIME format: '9999-12-31 23:59:59'\n",
    "- new process\n",
    "    - unpack h5\n",
    "        - Datasets -> arrays -> flatten into dataframe\n",
    "        - write dataframe to SQL with to_sql\n",
    "            - if you write two same-format dfs to_sql, does it append? i assume\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_orbit(h5): # filename string. adds h5's observations to dataframe\n",
    "    # the formatting/array shape is all uniform. thanks, NASA\n",
    "    f = h5py.File(h5, 'r') # read file\n",
    "    geo = f['GEOLOCATION_DATA'] # h5 Groups architecture is similar to dict\n",
    "    sci = f['SCIENCE_DATA']\n",
    "    # hdf.Datasets -> np.array -> flatten -> list. faster than looping through each matrix\n",
    "    lat = list(geo['Latitude'].value.ravel())\n",
    "    long = list(geo['Longitude'].value.ravel()) # is there a less verbose way to do this?\n",
    "    sat_lat = list(geo['SpacecraftLatitude'].value.ravel())*36 # extend the 1d arrays\n",
    "    sat_long = list(geo['SpacecraftLongitude'].value.ravel())*36 # don't forget to ravel like i did\n",
    "    sat_alt = list(geo['SpacecraftAltitude'].value.ravel())*36\n",
    "    time = list(geo['TimeUTC'].value.ravel())*36 # 36 measurements per position means one \"time\" value for every consecutive 36 measurements\n",
    "    sza = list(geo['SolarZenithAngle'].value.ravel())\n",
    "    pbl = list(sci['ColumnAmountSO2_PBL'].value.ravel())\n",
    "    anom = list(sci['Flag_SAA'].value.ravel())\n",
    "    volc = list(sci['Flag_SO2'].value.ravel())\n",
    "    # combine lists into df\n",
    "    new = pd.DataFrame(list(zip(lat, long, sat_lat, sat_long, sat_alt, time, sza, pbl, anom, volc)),\n",
    "                       columns=[\"lat\", \"long\", \"sat_lat\", \"sat_long\", \n",
    "                                'sat_alt', \"time\", \"sza\", \"pbl\", \"anom\", 'volc'])\n",
    "    new['time'] = new['time'].astype(str) # change time format\n",
    "    new['time'] = new['time'].apply(lambda st: st[2:12]+' '+st[13:21]) # change time format\n",
    "    return new # returns new df\n",
    "\n",
    "def get_files_list(): # gets list of filepath strings \n",
    "    mypath = os.getcwd()+'/data/'\n",
    "    files = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "    for f in files:\n",
    "        f = mypath+f # append full path for h5py loading\n",
    "    files = list(map(lambda f: mypath+f, files))\n",
    "    return files \n",
    "\n",
    "def process_h5s(files, processed, engine): # filepath strings, used filepath strings, sqlalchemy engine\n",
    "    # loads all h5s into dataframes -> uploads to SQL\n",
    "    # to keep track of progress\n",
    "    for f in tqdm(files):\n",
    "        df = process_orbit(f)\n",
    "        df.to_sql('so2', con=engine, if_exists='append') # sqlalchemy takes care of sessions, commits etc rather nicely\n",
    "#         processed.append(f.pop(0)) # move string to processed\n",
    "    return processed # is there a point to this if i can't interrupt the jupyter cell?\n",
    "# maybe i should have tracked 'files' and processed' in SQL tables? would have taken much longer though\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.dispose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## engine method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "create_engine() got an unexpected keyword argument 'echo'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-0652c7350d48>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mconnst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'mysql+pymysql://{user}:{pw}@{host}.clqqz5nrghvl.us-east-1.rds.amazonaws.com/so2'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mengine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mecho\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# don't set pool_recycle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mcreate_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-0652c7350d48>\u001b[0m in \u001b[0;36mcreate_engine\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mhost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mconnst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'mysql+pymysql://{user}:{pw}@{host}.clqqz5nrghvl.us-east-1.rds.amazonaws.com/so2'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mengine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mecho\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# don't set pool_recycle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mcreate_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: create_engine() got an unexpected keyword argument 'echo'"
     ]
    }
   ],
   "source": [
    "# create engine\n",
    "def ():\n",
    "    user = config.user\n",
    "    pw = config.pw\n",
    "    host = config.host\n",
    "    connst = f'mysql+pymysql://{user}:{pw}@{host}.clqqz5nrghvl.us-east-1.rds.amazonaws.com/so2'\n",
    "    engine = create_engine(connst, echo=False) # don't set pool_recycle\n",
    "()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38976"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get filepath strings list\n",
    "files = get_files_list()\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/38976 [00:00<?, ?it/s]/Users/mark/opt/anaconda3/lib/python3.7/site-packages/h5py/_hl/dataset.py:313: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "  \"Use dataset[()] instead.\", H5pyDeprecationWarning)\n",
      "100%|██████████| 38976/38976 [17:18:44<00:00,  1.60s/it]   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_h5s(files, processed, engine) # uploading 39k h5s will take a while..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# upload complete! (17 hours later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- but i dumped it to .sql file locally (86gb), and it looks like pandas doesn't have a native way to read mysqldump files; only streaming from live sql servers\n",
    "- i hope this doesn't take 17 more hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''SELECT * FROM so2'''\n",
    "start = time.time()\n",
    "df = pd.read_sql(query, engine)\n",
    "mid = time.time()\n",
    "print(f'streamed to DF in {mid-start}')\n",
    "print(f'writing csv ...')\n",
    "df.to_csv('all_so2.csv')\n",
    "end = time.time()\n",
    "print(f'data transferred in {end-start}') \n",
    "# workbench shows average speed 8mb/s, the sql dump was 86gb, so i'm assuming 3 hour runtime?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "307.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
